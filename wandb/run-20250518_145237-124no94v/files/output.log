Input dimension changed from 32 to 101 due to positional encoding.
encoder_dims: [32, 16, 8, 4, 2], decoder_dims: [2, 4, 8, 16, 32]
Vector quantization bottleneck dimension: 4
Epoch 1: 3it [00:01,  1.89it/s]
Encoder layer 0: torch.Size([8, 1, 220500, 32])
Encoder layer 1: torch.Size([8, 1, 220500, 16])
Encoder layer 2: torch.Size([8, 1, 220500, 8])
Encoder layer 3: torch.Size([8, 1, 220500, 4])
Encoder output: torch.Size([8, 1, 220500, 2])
Decoder layer 0: torch.Size([8, 1, 220500, 2])
Decoder layer 1: torch.Size([8, 1, 220500, 4])
Decoder layer 2: torch.Size([8, 1, 220500, 8])
Decoder layer 3: torch.Size([8, 1, 220500, 16])
Decoder output: torch.Size([8, 1, 220500, 32])
Encoder layer 0: torch.Size([8, 1, 220500, 32])
Encoder layer 1: torch.Size([8, 1, 220500, 16])
Encoder layer 2: torch.Size([8, 1, 220500, 8])
Encoder layer 3: torch.Size([8, 1, 220500, 4])
Encoder output: torch.Size([8, 1, 220500, 2])
Decoder layer 0: torch.Size([8, 1, 220500, 2])
Decoder layer 1: torch.Size([8, 1, 220500, 4])
Decoder layer 2: torch.Size([8, 1, 220500, 8])
Decoder layer 3: torch.Size([8, 1, 220500, 16])
Decoder output: torch.Size([8, 1, 220500, 32])
Encoder layer 0: torch.Size([8, 1, 220500, 32])
Encoder layer 1: torch.Size([8, 1, 220500, 16])
Encoder layer 2: torch.Size([8, 1, 220500, 8])
Encoder layer 3: torch.Size([8, 1, 220500, 4])
Encoder output: torch.Size([8, 1, 220500, 2])
Decoder layer 0: torch.Size([8, 1, 220500, 2])
Decoder layer 1: torch.Size([8, 1, 220500, 4])
Decoder layer 2: torch.Size([8, 1, 220500, 8])
Decoder layer 3: torch.Size([8, 1, 220500, 16])
Decoder output: torch.Size([8, 1, 220500, 32])
Encoder layer 0: torch.Size([8, 1, 220500, 32])
Encoder layer 1: torch.Size([8, 1, 220500, 16])
Encoder layer 2: torch.Size([8, 1, 220500, 8])
Encoder layer 3: torch.Size([8, 1, 220500, 4])
Encoder output: torch.Size([8, 1, 220500, 2])
Decoder layer 0: torch.Size([8, 1, 220500, 2])
Decoder layer 1: torch.Size([8, 1, 220500, 4])
Decoder layer 2: torch.Size([8, 1, 220500, 8])
Decoder layer 3: torch.Size([8, 1, 220500, 16])
Decoder output: torch.Size([8, 1, 220500, 32])
Encoder layer 0: torch.Size([8, 1, 220500, 32])
Encoder layer 1: torch.Size([8, 1, 220500, 16])
Encoder layer 2: torch.Size([8, 1, 220500, 8])
Encoder layer 3: torch.Size([8, 1, 220500, 4])
Encoder output: torch.Size([8, 1, 220500, 2])
Decoder layer 0: torch.Size([8, 1, 220500, 2])
Decoder layer 1: torch.Size([8, 1, 220500, 4])
Decoder layer 2: torch.Size([8, 1, 220500, 8])
Decoder layer 3: torch.Size([8, 1, 220500, 16])
Decoder output: torch.Size([8, 1, 220500, 32])
Encoder layer 0: torch.Size([8, 1, 220500, 32])
Encoder layer 1: torch.Size([8, 1, 220500, 16])
Encoder layer 2: torch.Size([8, 1, 220500, 8])
Encoder layer 3: torch.Size([8, 1, 220500, 4])
Encoder output: torch.Size([8, 1, 220500, 2])
Decoder layer 0: torch.Size([8, 1, 220500, 2])
Decoder layer 1: torch.Size([8, 1, 220500, 4])
Decoder layer 2: torch.Size([8, 1, 220500, 8])
Decoder layer 3: torch.Size([8, 1, 220500, 16])
Decoder output: torch.Size([8, 1, 220500, 32])
Encoder layer 0: torch.Size([8, 1, 220500, 32])
Encoder layer 1: torch.Size([8, 1, 220500, 16])
Encoder layer 2: torch.Size([8, 1, 220500, 8])
Encoder layer 3: torch.Size([8, 1, 220500, 4])
Encoder output: torch.Size([8, 1, 220500, 2])
Decoder layer 0: torch.Size([8, 1, 220500, 2])
Decoder layer 1: torch.Size([8, 1, 220500, 4])
Decoder layer 2: torch.Size([8, 1, 220500, 8])
Decoder layer 3: torch.Size([8, 1, 220500, 16])
Decoder output: torch.Size([8, 1, 220500, 32])
Encoder layer 0: torch.Size([8, 1, 220500, 32])
Encoder layer 1: torch.Size([8, 1, 220500, 16])
Encoder layer 2: torch.Size([8, 1, 220500, 8])
Encoder layer 3: torch.Size([8, 1, 220500, 4])
Encoder output: torch.Size([8, 1, 220500, 2])
Decoder layer 0: torch.Size([8, 1, 220500, 2])
Decoder layer 1: torch.Size([8, 1, 220500, 4])
Decoder layer 2: torch.Size([8, 1, 220500, 8])
Decoder layer 3: torch.Size([8, 1, 220500, 16])
Decoder output: torch.Size([8, 1, 220500, 32])
Encoder layer 0: torch.Size([8, 1, 220500, 32])
Encoder layer 1: torch.Size([8, 1, 220500, 16])
Encoder layer 2: torch.Size([8, 1, 220500, 8])
Encoder layer 3: torch.Size([8, 1, 220500, 4])
Encoder output: torch.Size([8, 1, 220500, 2])
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/amarjay/Desktop/code/neural_audio_modulation/neural_audio_modulation/src/training/train.py", line 100, in <module>
    train_model(config)
  File "/home/amarjay/Desktop/code/neural_audio_modulation/neural_audio_modulation/src/training/train.py", line 68, in train_model
    audio_output, embedding_loss, perplexity, _ = model(audio_input)
  File "/home/amarjay/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/amarjay/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/amarjay/Desktop/code/neural_audio_modulation/neural_audio_modulation/src/models/model.py", line 101, in forward
    embedding_loss, x, perplexity, _, _ = self.bottleneck(x)
  File "/home/amarjay/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/amarjay/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/amarjay/Desktop/code/neural_audio_modulation/neural_audio_modulation/src/models/quantization.py", line 57, in forward
    min_encodings = torch.zeros(min_encoding_indices.shape[0], self.n_e).to(device)
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/amarjay/Desktop/code/neural_audio_modulation/neural_audio_modulation/src/training/train.py", line 100, in <module>
    train_model(config)
  File "/home/amarjay/Desktop/code/neural_audio_modulation/neural_audio_modulation/src/training/train.py", line 68, in train_model
    audio_output, embedding_loss, perplexity, _ = model(audio_input)
  File "/home/amarjay/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/amarjay/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/amarjay/Desktop/code/neural_audio_modulation/neural_audio_modulation/src/models/model.py", line 101, in forward
    embedding_loss, x, perplexity, _, _ = self.bottleneck(x)
  File "/home/amarjay/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/amarjay/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/amarjay/Desktop/code/neural_audio_modulation/neural_audio_modulation/src/models/quantization.py", line 57, in forward
    min_encodings = torch.zeros(min_encoding_indices.shape[0], self.n_e).to(device)
KeyboardInterrupt
